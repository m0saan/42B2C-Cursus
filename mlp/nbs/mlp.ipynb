{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870814f-bebe-4ba3-abdc-e16334cc4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e1291-dec4-4cbc-9f16-5ed63b854321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"\n",
    "    A sequential container in Minima.\n",
    "\n",
    "    Modules will be added to it in the order they are passed in the constructor.\n",
    "    A `Sequential` module contains a sequence of child modules stored in the order they were added. \n",
    "    Each module is applied in order to the input to produce the output.\n",
    "\n",
    "    The `Sequential` class makes it easy to build networks where the output of one layer is the input to the next.\n",
    "\n",
    "    Attributes:\n",
    "    - `modules` (tuple of `Module`): The sequence of child modules to apply.\n",
    "\n",
    "    Methods:\n",
    "    - `forward(x: Tensor) -> Tensor`: Passes the input through all the child modules in sequential order.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        *modules # The sequence of child modules to apply. Each argument should be an instance of `Module`.\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new `Sequential` instance.\n",
    "        \n",
    "        Args:\n",
    "            *modules: The sequence of child modules to apply. Each argument should be an instance of `Module`.\n",
    "        \"\"\"\n",
    "        for i, module in enumerate(modules):\n",
    "            setattr(self, f'module_{i}', module)\n",
    "        self.modules = modules\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the sequential module.\n",
    "        \n",
    "        Passes the input through all the child modules in the order they were added.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        for module in self.modules:\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._iter_idx = 0;\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._iter_idx < len(self.modules):\n",
    "            res = self.modules[self._iter_idx]\n",
    "            self._iter_idx += 1\n",
    "            return res\n",
    "        raise StopIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f2130-d94b-489b-a0b5-6fadfe11c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(np.ndarray):\n",
    "    \"\"\" Parameter class\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d06f534-9599-4bf3-99ed-b0e448253d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unpack_params(value: object):\n",
    "    if isinstance(value, Parameter):\n",
    "        return [value]\n",
    "    elif isinstance(value, Module):\n",
    "        return list(value.parameters())\n",
    "    elif isinstance(value, list):\n",
    "        return [item for v in value for item in _unpack_params(v)]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f53220-625c-4d5b-83ee-7a208149804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list of all `Parameter` instances in the module.\n",
    "        This is done by unpacking the parameters from the module's dictionary.\n",
    "        \"\"\"\n",
    "        return _unpack_params(self.__dict__)\n",
    "\n",
    "    def _children(self):\n",
    "        \"\"\"\n",
    "        Returns a list of all child `Module` instances in the module.\n",
    "        This is done by unpacking the modules from the module's dictionary.\n",
    "        \"\"\"\n",
    "        return _child_modules(self.__dict__)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Defines the call method for the module.\n",
    "        This method simply calls the forward method and must be overridden by all subclasses.\n",
    "        \"\"\"\n",
    "        self.input = args\n",
    "        self.output = self.forward(*args, **kwargs)\n",
    "    \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7732484-a0df-4a93-bf1d-d871149f2a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A class representing a fully connected (linear) layer in a neural network.\n",
    "    This class inherits from the `Module` class.\n",
    "\n",
    "    Attributes:\n",
    "        in_features (int): The number of input features.\n",
    "        out_features (int): The number of output features.\n",
    "        device (str): The device to store the Parameters on (defaults to None, which means CPU).\n",
    "        dtype (str): The data type of the Parameters (defaults to 'float32').\n",
    "        weight (Parameter): The weight parameters of the layer.\n",
    "        bias (Parameter): The bias parameters of the layer, or None if bias=False.\n",
    "\n",
    "    Methods:\n",
    "        forward(X: Tensor) -> Tensor: Compute the forward pass of the layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features, # The number of input features.\n",
    "        out_features,# The number of output features.\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the layer with given input/output feature sizes and, optionally, bias, device, and dtype.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): The number of input features.\n",
    "            out_features (int): The number of output features.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weight = 0.01 * np.random.randn(in_features, out_features)\n",
    "        self.bias = np.zeros((1, out_features))\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f'Linear(in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None})'\n",
    "            \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the layer.\n",
    "\n",
    "        This function applies the linear transformation to the input tensor X, \n",
    "        i.e., performs the matrix multiplication of X and the weight tensor, \n",
    "        and then adds the bias tensor (if bias is not None).\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        out = X @ self.weight + self.bias\n",
    "        self.inputs = X; self.outputs = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        # Gradients on parameters\n",
    "        self.weight_grad = np.dot(self.inputs.T, grad)\n",
    "        self.bias_grad = np.sum(grad, axis=0, keepdims=True)\n",
    "    \n",
    "        # Gradient on inputs\n",
    "        self.inputs_grad = np.dot(grad, self.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f6c40-0c5b-4a5e-a360-29a76fa883ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.inputs = X\n",
    "        self.outputs = np.maximum(x, 0)\n",
    "        return self.outputs\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        self.inputs_grad = grad.copy()\n",
    "        self.inputs_grad[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008dcfcc-d32e-4c88-bd11-5820f75da2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    def forward(self, X):\n",
    "        exp_vals = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        probs = exp_vals / np.sum(exp_vals, axis=1, keepdims=True); self.outputs = probs\n",
    "        return probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c1bf88-91a6-4dd1-bfe4-c2d87d264b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.inputs = X\n",
    "        self.outputs = 1 / (1 + np.exp(-X))\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, grad):\n",
    "        self.inputs_grad = grad * (1 - self.outputs) * self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d97881-abfd-40cf-a95b-c81a5b3a9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()\n",
    "\n",
    "def log_softmax(x): return x - logsumexp(x)\n",
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce7189c-52ca-418a-b77b-6e7e9aa07cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6500d9-b566-40b0-9445-183c218288e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Module):\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "       return nll(y_hat, y)\n",
    "\n",
    "    def backward(self, grad, y):\n",
    "    \n",
    "        # Number of labels in every sample\n",
    "        num_labels = len(grad[0])\n",
    "        \n",
    "        # If labels are sparse, turn them into one-hot vectors\n",
    "        if len(y_true.shape) == 1:\n",
    "            y = np.eye(num_labels)[y]\n",
    "    \n",
    "        self.inputs_grad = (-y / grad) / len(grad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e9260-8693-4441-bdd8-c45d9dbfb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy(Module):\n",
    "    def __init__(self):\n",
    "        self.inputs = None\n",
    "        self.targets = None\n",
    "        self.dinputs = None\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        \n",
    "        # Avoid division by zero and clip values for numerical stability\n",
    "        epsilon = 1e-7\n",
    "        inputs = np.clip(inputs, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Calculate binary cross-entropy loss\n",
    "        loss = -np.mean(targets * np.log(inputs) + (1 - targets) * np.log(1 - inputs))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples and outputs\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -((y_true / clipped_dvalues) - ((1 - y_true) / (1 - clipped_dvalues))) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7687ea-d6b3-4ea2-85c2-8cddc7740fb4",
   "metadata": {},
   "source": [
    "# Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9faacd-dda2-4eb8-83b4-ab45a70d1b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    Base class for all optimizers. Not meant to be instantiated directly.\n",
    "\n",
    "    This class represents the abstract concept of an optimizer, and contains methods that \n",
    "    all concrete optimizer classes must implement. It is designed to handle the parameters \n",
    "    of a machine learning model, providing functionality to perform a step of optimization \n",
    "    and to zero out gradients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : Iterable\n",
    "        The parameters of the model to be optimized.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        If the `step` method is not implemented in a subclass.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers # The parameters of the model to be optimized.\n",
    "    ):\n",
    "        self.layers = layers\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        This method must be overridden by any subclass to provide the specific optimization logic.\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            If the method is not implemented in a subclass.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40407d4-2361-4fe1-b251-351a9dfeed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements stochastic gradient descent (optionally with momentum).\n",
    "\n",
    "    This is a basic optimizer that's suitable for many machine learning models, and is often\n",
    "    used as a baseline for comparing other optimizers' performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : Iterable\n",
    "        The parameters of the model to be optimized.\n",
    "    lr : float, optional\n",
    "        The learning rate.\n",
    "    momentum : float, optional\n",
    "        The momentum factor.\n",
    "    wd : float, optional\n",
    "        The weight decay (L2 regularization).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers, # The parameters of the model to be optimized.\n",
    "        lr=0.01, # The learning rate.\n",
    "    ):\n",
    "        super().__init__(layers)\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        This method uses the current gradients to adjust the parameters using stochastic gradient descent.\n",
    "        \"\"\"\n",
    "        for self.idx, layer in enumerate(self.layers):\n",
    "            self._opt_step(layer)\n",
    "\n",
    "    def _opt_step(self, layer):\n",
    "        \"\"\"\n",
    "        Performs the optimization step for a single parameter tensor.\n",
    "\n",
    "        If momentum is set, it applies momentum by using a running average of the previous gradients.\n",
    "        \"\"\"\n",
    "\n",
    "        layer.weight -= self.lr * layer.weight_grad\n",
    "        layer.bias -= self.lr * layer.bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302fbcf-29b2-480c-a761-5ca1b4a48a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa0283-29a2-4e3b-bd97-4d7f142aea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.dense1 = nn.Linear(in_features=input_shape, out_features=24)\n",
    "        self.dense2 = nn.Linear(24, 24)\n",
    "        self.dense3 = nn.Linear(24, 24)\n",
    "        self.dense4 = nn.Linear(24, output_shape)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.dense1(x))\n",
    "        x = F.sigmoid(self.dense2(x))\n",
    "        x = F.sigmoid(self.dense3(x))\n",
    "        x = F.softmax(self.dense4(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(module.weight)\n",
    "\n",
    "# Create the neural network\n",
    "input_shape = 30  # Replace with the actual input shape\n",
    "output_shape = 2  # Replace with the actual output shape\n",
    "\n",
    "network = NeuralNetwork(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b5a3d-fee8-4809-9e3c-ee9af555f568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (dense1): Linear(in_features=30, out_features=24, bias=True)\n",
       "  (dense2): Linear(in_features=24, out_features=24, bias=True)\n",
       "  (dense3): Linear(in_features=24, out_features=24, bias=True)\n",
       "  (dense4): Linear(in_features=24, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361f6f5-91b7-47cb-b781-b47a556ceb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset):\n",
    "    # Compute the mean and standard deviation along the axis 0 (columns)\n",
    "    mean = np.mean(dataset, axis=0)\n",
    "    std = np.std(dataset, axis=0)\n",
    "\n",
    "    # Normalize the dataset by subtracting the mean and dividing by the standard deviation\n",
    "    normalized_dataset = (dataset - mean) / std\n",
    "\n",
    "    return normalized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60b759-1346-4867-9038-5e8a1e39197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def data_split(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    X = df.iloc[:, 2:].values\n",
    "    y = df.iloc[:, 1].values\n",
    "    y = np.where(y == 'M', 1, 0)\n",
    "\n",
    "    X = normalize_dataset(X)\n",
    "    \n",
    "    split_ratio = 0.8\n",
    "    split_index = int(split_ratio * len(X))\n",
    "    X_tr, y_tr = X[:split_index], y[:split_index]\n",
    "    X_val, y_val = X[split_index:], y[split_index:]\n",
    "    return X_tr, y_tr, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f66ee-6787-4036-b821-71d22d1ff7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr, X_val, y_val = data_split('./data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c55d9-8eaa-46da-abf5-7603932c498a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(454, 30)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7aa256-151a-47df-a35e-0c530c9e0f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "# t = partial(torch.tensor, dtype=torch.float32)\n",
    "# X_tr, y_tr, X_val, y_val = map(t, (X_tr, y_tr, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a8327-9b1c-4e3b-bdcc-edeb5971e36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((454, 30), dtype('float64'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_tr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3590383-e4a8-4424-bf79-42fe5eb76d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6764, 0.3236],\n",
       "        [0.6763, 0.3237],\n",
       "        [0.6864, 0.3136],\n",
       "        [0.6690, 0.3310],\n",
       "        [0.6832, 0.3168],\n",
       "        [0.6814, 0.3186],\n",
       "        [0.6782, 0.3218],\n",
       "        [0.6815, 0.3185],\n",
       "        [0.6852, 0.3148],\n",
       "        [0.6824, 0.3176]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network(X_tr)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451290a-57a5-41c6-bf7d-0c0821f8610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X: tensor([[-0.0996, -1.4086, -0.1589, -0.2036, -0.3094, -0.7996, -0.9820, -0.7666,\n",
      "         -0.8007, -0.5192, -0.6703, -0.9480, -0.6859, -0.4951, -0.8973, -0.9666,\n",
      "         -0.9142, -0.8042,  0.1229, -0.7521, -0.3286, -1.4279, -0.3874, -0.3833,\n",
      "         -0.6718, -0.9358, -1.1265, -0.8608, -0.1217, -0.8857],\n",
      "        [-1.0966, -1.0725, -1.0598, -0.9462,  0.1780, -0.2379, -0.6642, -0.7355,\n",
      "         -0.6505,  1.0358, -0.2392,  2.3420, -0.2301, -0.4599,  3.4366,  1.1897,\n",
      "         -0.1904,  0.1899,  1.9650,  1.1222, -1.1152, -1.0123, -1.0834, -0.9192,\n",
      "          0.1616, -0.5751, -0.9614, -1.1249, -0.7546,  0.0553],\n",
      "        [-1.0455, -0.8975, -1.0425, -0.9234,  0.6398, -0.5126, -1.0323, -0.9482,\n",
      "         -0.0826,  0.2462, -0.7711, -0.5706, -0.8209, -0.6250, -0.1762, -0.9717,\n",
      "         -0.9624, -1.1052, -0.6668, -0.3667, -0.9824, -0.9602, -1.0063, -0.8516,\n",
      "          0.0782, -0.8840, -1.1791, -1.0653, -0.4601, -0.0663],\n",
      "        [ 1.5423,  2.2089,  1.7180,  1.5720, -0.2652,  1.9550,  1.1357,  1.7016,\n",
      "          0.3021, -0.0640,  1.5905, -0.0334,  2.1430,  1.4646, -0.2285,  1.2423,\n",
      "          0.2245,  0.5514, -0.1412,  0.5527,  1.8258,  1.7239,  2.1371,  1.8462,\n",
      "         -0.1849,  1.7864,  0.7396,  1.2191, -0.1298,  0.9230]])\n",
      "Batch y: tensor([0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "tr_ds = MyDataset(X_tr, y_tr)\n",
    "val_ds = MyDataset(X_val, y_val)\n",
    "\n",
    "# Creating the data loader\n",
    "batch_size = 4\n",
    "tr_dl = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# Iterating over the data loader\n",
    "for batch_X, batch_y in tr_dl:\n",
    "    print(\"Batch X:\", batch_X)\n",
    "    print(\"Batch y:\", batch_y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dbc960-2ef5-485b-b5ee-114a345e1619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01/70 - loss: 0.6857 - acc: 0.5921 - val_loss: 0.5925 - val_acc: 0.7684\n",
      "epoch 02/70 - loss: 0.6837 - acc: 0.5921 - val_loss: 0.5960 - val_acc: 0.7750\n",
      "epoch 03/70 - loss: 0.6833 - acc: 0.5899 - val_loss: 0.5980 - val_acc: 0.7772\n",
      "epoch 04/70 - loss: 0.6813 - acc: 0.5921 - val_loss: 0.6003 - val_acc: 0.7772\n",
      "epoch 05/70 - loss: 0.6800 - acc: 0.5921 - val_loss: 0.6062 - val_acc: 0.7706\n",
      "epoch 06/70 - loss: 0.6782 - acc: 0.5943 - val_loss: 0.6065 - val_acc: 0.7728\n",
      "epoch 07/70 - loss: 0.6782 - acc: 0.5921 - val_loss: 0.6067 - val_acc: 0.7728\n",
      "epoch 08/70 - loss: 0.6773 - acc: 0.5921 - val_loss: 0.6068 - val_acc: 0.7706\n",
      "epoch 09/70 - loss: 0.6763 - acc: 0.5921 - val_loss: 0.6046 - val_acc: 0.7794\n",
      "epoch 10/70 - loss: 0.6745 - acc: 0.5943 - val_loss: 0.6048 - val_acc: 0.7706\n",
      "epoch 11/70 - loss: 0.6741 - acc: 0.5921 - val_loss: 0.6071 - val_acc: 0.7706\n",
      "epoch 12/70 - loss: 0.6741 - acc: 0.5899 - val_loss: 0.6049 - val_acc: 0.7750\n",
      "epoch 13/70 - loss: 0.6721 - acc: 0.5921 - val_loss: 0.6033 - val_acc: 0.7728\n",
      "epoch 14/70 - loss: 0.6701 - acc: 0.5943 - val_loss: 0.6017 - val_acc: 0.7706\n",
      "epoch 15/70 - loss: 0.6691 - acc: 0.5943 - val_loss: 0.6000 - val_acc: 0.7706\n",
      "epoch 16/70 - loss: 0.6686 - acc: 0.5921 - val_loss: 0.5956 - val_acc: 0.7816\n",
      "epoch 17/70 - loss: 0.6663 - acc: 0.5943 - val_loss: 0.5985 - val_acc: 0.7662\n",
      "epoch 18/70 - loss: 0.6654 - acc: 0.5921 - val_loss: 0.6068 - val_acc: 0.7575\n",
      "epoch 19/70 - loss: 0.6650 - acc: 0.5899 - val_loss: 0.6021 - val_acc: 0.7750\n",
      "epoch 20/70 - loss: 0.6618 - acc: 0.5943 - val_loss: 0.6018 - val_acc: 0.7597\n",
      "epoch 21/70 - loss: 0.6597 - acc: 0.5943 - val_loss: 0.5955 - val_acc: 0.7706\n",
      "epoch 22/70 - loss: 0.6577 - acc: 0.5943 - val_loss: 0.5935 - val_acc: 0.7728\n",
      "epoch 23/70 - loss: 0.6566 - acc: 0.5921 - val_loss: 0.5948 - val_acc: 0.7706\n",
      "epoch 24/70 - loss: 0.6538 - acc: 0.5921 - val_loss: 0.5863 - val_acc: 0.7837\n",
      "epoch 25/70 - loss: 0.6510 - acc: 0.5921 - val_loss: 0.5870 - val_acc: 0.7706\n",
      "epoch 26/70 - loss: 0.6487 - acc: 0.5921 - val_loss: 0.5893 - val_acc: 0.7772\n",
      "epoch 27/70 - loss: 0.6452 - acc: 0.5921 - val_loss: 0.5921 - val_acc: 0.7728\n",
      "epoch 28/70 - loss: 0.6412 - acc: 0.5943 - val_loss: 0.5874 - val_acc: 0.7728\n",
      "epoch 29/70 - loss: 0.6377 - acc: 0.5921 - val_loss: 0.5839 - val_acc: 0.7728\n",
      "epoch 30/70 - loss: 0.6325 - acc: 0.5943 - val_loss: 0.5808 - val_acc: 0.7728\n",
      "epoch 31/70 - loss: 0.6280 - acc: 0.5921 - val_loss: 0.5750 - val_acc: 0.7728\n",
      "epoch 32/70 - loss: 0.6226 - acc: 0.6053 - val_loss: 0.5709 - val_acc: 0.7772\n",
      "epoch 33/70 - loss: 0.6172 - acc: 0.6009 - val_loss: 0.5708 - val_acc: 0.8262\n",
      "epoch 34/70 - loss: 0.6103 - acc: 0.6952 - val_loss: 0.5643 - val_acc: 0.8641\n",
      "epoch 35/70 - loss: 0.6033 - acc: 0.7149 - val_loss: 0.5637 - val_acc: 0.9309\n",
      "epoch 36/70 - loss: 0.5945 - acc: 0.7851 - val_loss: 0.5566 - val_acc: 0.9387\n",
      "epoch 37/70 - loss: 0.5861 - acc: 0.8202 - val_loss: 0.5492 - val_acc: 0.9487\n",
      "epoch 38/70 - loss: 0.5767 - acc: 0.8421 - val_loss: 0.5417 - val_acc: 0.9544\n",
      "epoch 39/70 - loss: 0.5673 - acc: 0.8596 - val_loss: 0.5364 - val_acc: 0.9531\n",
      "epoch 40/70 - loss: 0.5565 - acc: 0.8816 - val_loss: 0.5298 - val_acc: 0.9487\n",
      "epoch 41/70 - loss: 0.5463 - acc: 0.9079 - val_loss: 0.5127 - val_acc: 0.9587\n",
      "epoch 42/70 - loss: 0.5359 - acc: 0.8969 - val_loss: 0.5144 - val_acc: 0.9366\n",
      "epoch 43/70 - loss: 0.5245 - acc: 0.9079 - val_loss: 0.5019 - val_acc: 0.9409\n",
      "epoch 44/70 - loss: 0.5135 - acc: 0.9123 - val_loss: 0.4972 - val_acc: 0.9309\n",
      "epoch 45/70 - loss: 0.5038 - acc: 0.9298 - val_loss: 0.4876 - val_acc: 0.9409\n",
      "epoch 46/70 - loss: 0.4936 - acc: 0.9276 - val_loss: 0.4769 - val_acc: 0.9431\n",
      "epoch 47/70 - loss: 0.4833 - acc: 0.9386 - val_loss: 0.4720 - val_acc: 0.9166\n",
      "epoch 48/70 - loss: 0.4755 - acc: 0.9386 - val_loss: 0.4653 - val_acc: 0.9287\n",
      "epoch 49/70 - loss: 0.4661 - acc: 0.9408 - val_loss: 0.4574 - val_acc: 0.9287\n",
      "epoch 50/70 - loss: 0.4579 - acc: 0.9408 - val_loss: 0.4471 - val_acc: 0.9331\n",
      "epoch 51/70 - loss: 0.4508 - acc: 0.9408 - val_loss: 0.4432 - val_acc: 0.9287\n",
      "epoch 52/70 - loss: 0.4439 - acc: 0.9430 - val_loss: 0.4406 - val_acc: 0.9287\n",
      "epoch 53/70 - loss: 0.4377 - acc: 0.9430 - val_loss: 0.4343 - val_acc: 0.9353\n",
      "epoch 54/70 - loss: 0.4318 - acc: 0.9430 - val_loss: 0.4327 - val_acc: 0.9387\n",
      "epoch 55/70 - loss: 0.4265 - acc: 0.9430 - val_loss: 0.4259 - val_acc: 0.9387\n",
      "epoch 56/70 - loss: 0.4216 - acc: 0.9430 - val_loss: 0.4218 - val_acc: 0.9387\n",
      "epoch 57/70 - loss: 0.4170 - acc: 0.9430 - val_loss: 0.4171 - val_acc: 0.9409\n",
      "epoch 58/70 - loss: 0.4129 - acc: 0.9430 - val_loss: 0.4107 - val_acc: 0.9487\n",
      "epoch 59/70 - loss: 0.4095 - acc: 0.9430 - val_loss: 0.4109 - val_acc: 0.9487\n",
      "epoch 60/70 - loss: 0.4053 - acc: 0.9452 - val_loss: 0.4055 - val_acc: 0.9509\n",
      "epoch 61/70 - loss: 0.4021 - acc: 0.9452 - val_loss: 0.4018 - val_acc: 0.9487\n",
      "epoch 62/70 - loss: 0.3996 - acc: 0.9452 - val_loss: 0.4036 - val_acc: 0.9466\n",
      "epoch 63/70 - loss: 0.3958 - acc: 0.9474 - val_loss: 0.3997 - val_acc: 0.9466\n",
      "epoch 64/70 - loss: 0.3939 - acc: 0.9496 - val_loss: 0.3966 - val_acc: 0.9466\n",
      "epoch 65/70 - loss: 0.3910 - acc: 0.9496 - val_loss: 0.3961 - val_acc: 0.9466\n",
      "epoch 66/70 - loss: 0.3879 - acc: 0.9539 - val_loss: 0.3913 - val_acc: 0.9466\n",
      "epoch 67/70 - loss: 0.3857 - acc: 0.9561 - val_loss: 0.3869 - val_acc: 0.9587\n",
      "epoch 68/70 - loss: 0.3836 - acc: 0.9539 - val_loss: 0.3860 - val_acc: 0.9566\n",
      "epoch 69/70 - loss: 0.3813 - acc: 0.9583 - val_loss: 0.3855 - val_acc: 0.9544\n",
      "epoch 70/70 - loss: 0.3820 - acc: 0.9561 - val_loss: 0.3845 - val_acc: 0.9644\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "network = NeuralNetwork(input_shape, output_shape)\n",
    "opt = torch.optim.SGD(network.parameters(), lr=0.01)\n",
    "bce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "network.train()\n",
    "num_epochs = 70\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Training phase\n",
    "    network.train()\n",
    "    for xb, yb in tr_dl:\n",
    "        preds = network(xb)\n",
    "        loss = bce(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted_labels = torch.max(preds, dim=1)\n",
    "        accuracy = (predicted_labels == yb).sum().item() / yb.size(0)\n",
    "        train_accs.append(accuracy)\n",
    "    \n",
    "    # Validation phase\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb_val, yb_val in val_dl:\n",
    "            preds_val = network(xb_val)\n",
    "            val_loss = bce(preds_val, yb_val)\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted_labels_val = torch.max(preds_val, dim=1)\n",
    "            accuracy_val = (predicted_labels_val == yb_val).sum().item() / yb_val.size(0)\n",
    "            val_accs.append(accuracy_val)\n",
    "    \n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    avg_train_acc = sum(train_accs) / len(train_accs)\n",
    "    avg_val_acc = sum(val_accs) / len(val_accs)\n",
    "    \n",
    "    # Print epoch-wise loss and accuracy\n",
    "    print(f\"epoch {epoch + 1:02d}/{num_epochs:02d} - loss: {avg_train_loss:.4f} - acc: {avg_train_acc:.4f} - val_loss: {avg_val_loss:.4f} - val_acc: {avg_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efb377-2160-4995-b433-2bc23055d539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
