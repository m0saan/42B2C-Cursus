{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c084b2-9ae7-4fd0-936c-001c9ad28f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minima as mi\n",
    "from minima.data import Dataset, Sampler, BatchSampler\n",
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2431a-561b-4898-a42c-6d72509e5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def data_split(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    X = df.iloc[:, 2:].values\n",
    "    y = df.iloc[:, 1].values\n",
    "    y = np.where(y == 'M', 1, 0)\n",
    "\n",
    "    X = normalize_dataset(X)\n",
    "    \n",
    "    split_ratio = 0.8\n",
    "    split_index = int(split_ratio * len(X))\n",
    "    X_tr, y_tr = X[:split_index], y[:split_index]\n",
    "    X_val, y_val = X[split_index:], y[split_index:]\n",
    "    return X_tr, y_tr, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a77424-b24f-4bd0-832b-7159b1bfebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset):\n",
    "    # Compute the mean and standard deviation along the axis 0 (columns)\n",
    "    mean = np.mean(dataset, axis=0)\n",
    "    std = np.std(dataset, axis=0)\n",
    "\n",
    "    # Normalize the dataset by subtracting the mean and dividing by the standard deviation\n",
    "    normalized_dataset = (dataset - mean) / std\n",
    "\n",
    "    return normalized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab01b41-d623-46e0-9c40-feaaf4c01192",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr, X_val, y_val = data_split('./data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbce24c-b22a-4475-ab27-01c9d74eef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs,ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    A custom data loader class.\n",
    "\n",
    "    Args:\n",
    "        ds (Dataset): The dataset to load.\n",
    "        bs (int): Batch size.\n",
    "\n",
    "    Example:\n",
    "        >>> dataloader = DataLoader(dataset, batch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset: Dataset,\n",
    "                 batch_size: int = 1,\n",
    "                 shuffle: bool = True,\n",
    "                 sampler: Sampler = None,\n",
    "                 batch_sampler: BatchSampler = None,\n",
    "                 num_workers: int = 0,\n",
    "                 collate_fn: callable = None,\n",
    "                 drop_last: bool = False):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.sampler = sampler if sampler else Sampler(dataset, shuffle)\n",
    "        self.batch_sampler = batch_sampler if batch_sampler else BatchSampler(self.sampler, batch_size, drop_last)\n",
    "        self.num_workers = num_workers # --> TODO: implement a multiprocessing DataLoader :3\n",
    "        self.collate_fn = collate\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Get an iterator over the DataLoader.\n",
    "\n",
    "        Yields:\n",
    "            Tuple[float, float]: A tuple containing a batch of input data and target labels.\n",
    "\n",
    "        Example:\n",
    "            >>> for batch in dataloader:\n",
    "            >>>     # Process the batch\n",
    "        \"\"\"\n",
    "        if self.num_workers:\n",
    "            with mp.Pool(self.num_workers) as ex:\n",
    "                yield from ex.map(self.dataset.__getitem__,  iter(self.batch_sampler))\n",
    "        else:\n",
    "            yield from (self.dataset[batch_idxs] for batch_idxs in self.batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b76319-9125-402a-ad84-8386c15bebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = mi.Tensor(X)\n",
    "        self.y = mi.Tensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "tr_ds = MyDataset(X_tr, y_tr)\n",
    "val_ds = MyDataset(X_val, y_val)\n",
    "\n",
    "# Creating the data loader\n",
    "batch_size = 2\n",
    "tr_dl = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b7519-8111-4b56-9008-807982f7890b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.DataLoader"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc272af-75bc-4960-8d04-ead67fd0bbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X: tensor([[-0.323974  0.010424 -0.304757 -0.39896  -0.037872  0.161808 -0.21909  -0.498826 -0.18153   0.37424  -0.491515 -1.180435 -0.19476  -0.430746 -0.43802\n",
      "   0.39782   0.400195 -0.320686  0.845901  0.295489 -0.380474 -0.609805 -0.236657 -0.430564 -0.154227  0.458861  0.348815 -0.240892  1.136195  0.431948]\n",
      " [-0.181942  0.353503 -0.144852 -0.27041   0.376133  0.410331  0.225596  0.146274 -0.331739  0.202087 -0.692389 -1.135104 -0.653037 -0.477825 -0.557924\n",
      "  -0.170387 -0.045249  0.134735 -0.818189 -0.228308 -0.150091  0.053466  0.005231 -0.243553  1.258175  1.080403  1.114435  1.703578 -0.147691  1.289654]])\n",
      "Batch y: tensor([0 1])\n",
      "<class 'minima.autograd.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# # Iterating over the data loader\n",
    "for batch_X, batch_y in tr_dl:\n",
    "    print(\"Batch X:\", batch_X)\n",
    "    print(\"Batch y:\", batch_y)\n",
    "    print(type(batch_X))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0b7f7-67c0-479d-aeb2-17d92517ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minima.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbca296-fb03-4751-b7d9-43fad6300fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.dense1 = nn.Linear(in_features=input_shape, out_features=24)\n",
    "        self.dense2 = nn.Linear(24, 24)\n",
    "        self.dense3 = nn.Linear(24, 24)\n",
    "        self.dense4 = nn.Linear(24, output_shape)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.relu(self.dense2(x))\n",
    "        x = self.relu(self.dense3(x))\n",
    "        # print(self.dense4(x))\n",
    "        x = self.dense4(x)\n",
    "        return x\n",
    "\n",
    "# Create the neural network\n",
    "input_shape = 30  # Replace with the actual input shape\n",
    "output_shape = 2  # Replace with the actual output shape\n",
    "\n",
    "network = NeuralNetwork(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce06e8-35bc-478d-92d9-6093b8a4c8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (dense1): Linear(in_features=30, out_features=24, bias=True)\n",
       "  (dense2): Linear(in_features=24, out_features=24, bias=True)\n",
       "  (dense3): Linear(in_features=24, out_features=24, bias=True)\n",
       "  (dense4): Linear(in_features=24, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b2d9f-9075-47b3-b6b6-0705ddf79ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minima import optim\n",
    "def net():\n",
    "    network = NeuralNetwork(input_shape, output_shape)\n",
    "    opt = optim.SGD(network.parameters(), lr=0.01)\n",
    "    bce = nn.CrossEntropyLoss()\n",
    "    \n",
    "    network.train()\n",
    "    num_epochs = 70\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        \n",
    "        # Training phase\n",
    "        network.train()\n",
    "        for xb, yb in tr_dl:\n",
    "            preds = network(xb)\n",
    "            loss = bce(preds, yb)\n",
    "            # import pdb; pdb.set_trace()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            # _, predicted_labels = torch.max(preds, dim=1)\n",
    "            accuracy = (predicted_labels == yb).sum().item() / yb.size(0)\n",
    "            train_accs.append(accuracy)\n",
    "        \n",
    "        # Validation phase\n",
    "        network.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb_val, yb_val in val_dl:\n",
    "                preds_val = network(xb_val)\n",
    "                val_loss = bce(preds_val, yb_val)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted_labels_val = torch.max(preds_val, dim=1)\n",
    "                accuracy_val = (predicted_labels_val == yb_val).sum().item() / yb_val.size(0)\n",
    "                val_accs.append(accuracy_val)\n",
    "        \n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        avg_train_acc = sum(train_accs) / len(train_accs)\n",
    "        avg_val_acc = sum(val_accs) / len(val_accs)\n",
    "        \n",
    "        # Print epoch-wise loss and accuracy\n",
    "        print(f\"epoch {epoch + 1:02d}/{num_epochs:02d} - loss: {avg_train_loss:.4f} - acc: {avg_train_acc:.4f} - val_loss: {avg_val_loss:.4f} - val_acc: {avg_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c32375-5ba1-4a80-9a89-9058e642feb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m, in \u001b[0;36mnet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m _, predicted_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mmax(preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (predicted_labels \u001b[38;5;241m==\u001b[39m yb)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m yb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m train_accs\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa35ffb-5898-4eca-b15d-3c4df841916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi.operators.logsumexp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae452f5-8535-4e51-8bc2-a4900f41d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef49a6-833b-4dad-957b-07b42ba85b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1,3]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e939d-1df7-4bdd-aae1-25f681014a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ffb96-bce7-494b-8cd5-15b1c9dc8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53ce27-2724-4749-a47d-718c09273742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def default_collate(batch):\n",
    "    if isinstance(batch[0], torch.Tensor):\n",
    "        return torch.stack(batch, dim=0)\n",
    "    elif isinstance(batch[0], int):\n",
    "        return torch.tensor(batch, dtype=torch.int64)\n",
    "    elif isinstance(batch[0], float):\n",
    "        return torch.tensor(batch, dtype=torch.float32)\n",
    "    elif isinstance(batch[0], str):\n",
    "        return batch\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported data type encountered in default_collate\")\n",
    "\n",
    "# Usage example:\n",
    "data = [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6])]\n",
    "collated_data = default_collate(data)\n",
    "print(collated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf24898-06c8-4b3a-9f8e-71f127386b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
