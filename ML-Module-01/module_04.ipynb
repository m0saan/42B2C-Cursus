{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2389c9-a4f6-44e8-b0cd-c9f984b5f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfffdbb-db4b-4d2b-8904-4b8b34da0b98",
   "metadata": {},
   "source": [
    "# Exercise 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a2027-8c7c-4ccf-aa77-b88016c3fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polynomial_features(x, power):\n",
    "    \"\"\"\n",
    "    Add polynomial features to matrix x by raising its columns to every power in the range of 1 up to the power giveArgs:\n",
    "    x: has to be an numpy.ndarray, a matrix of shape m * n.\n",
    "    power: has to be an int, the power up to which the columns of matrix x are going to be raised.\n",
    "    \n",
    "    Returns:\n",
    "    The matrix of polynomial features as a numpy.ndarray, of shape m * (np), containg the polynomial feature vaNone if x is an empty numpy.ndarray.\n",
    "    \n",
    "    Raises:\n",
    "    This function should not raise any Exception.\n",
    "    \"\"\"\n",
    "\n",
    "    out = [x]\n",
    "    for i in range(2, power+1):\n",
    "        out.append(x ** i)\n",
    "    return np.hstack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4b150-5d1b-497f-8958-a64c6842d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,11).reshape(5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2391a8-0807-43af-9a3d-31f79d7be641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    2,    1,    4,    1,    8],\n",
       "       [   3,    4,    9,   16,   27,   64],\n",
       "       [   5,    6,   25,   36,  125,  216],\n",
       "       [   7,    8,   49,   64,  343,  512],\n",
       "       [   9,   10,   81,  100,  729, 1000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_polynomial_features(x, 3)\n",
    "# Output:\n",
    "# array([[ 1, 2, 1, 4, 1, 8],\n",
    "# [ 3, 4, 9, 16, 27, 64],\n",
    "# [ 5, 6, 25, 36, 125, 216],\n",
    "# [ 7, 8, 49, 64, 343, 512],\n",
    "# [ 9, 10, 81, 100, 729, 1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c020b-0695-4eb9-802b-22d8cdcc27b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     2,     1,     4,     1,     8,     1,    16],\n",
       "       [    3,     4,     9,    16,    27,    64,    81,   256],\n",
       "       [    5,     6,    25,    36,   125,   216,   625,  1296],\n",
       "       [    7,     8,    49,    64,   343,   512,  2401,  4096],\n",
       "       [    9,    10,    81,   100,   729,  1000,  6561, 10000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_polynomial_features(x, 4)\n",
    "# Output:\n",
    "# array([[ 1, 2, 1, 4, 1, 8, 1, 16],\n",
    "# [ 3, 4, 9, 16, 27, 64, 81, 256],\n",
    "# [ 5, 6, 25, 36, 125, 216, 625, 1296],\n",
    "# [ 7, 8, 49, 64, 343, 512, 2401, 4096],\n",
    "# [ 9, 10, 81, 100, 729, 1000, 6561, 10000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84cbcaa-b7a7-42e4-9f93-1abd6cdedd9e",
   "metadata": {},
   "source": [
    "# Exercise 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f3110-c864-47f8-b5d5-a99c2a8280d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_l2(theta):\n",
    "    \"\"\"\n",
    "    Computes the L2 regularization of a non-empty numpy.ndarray, with a for-loop.\n",
    "    \n",
    "    Args:\n",
    "    theta: has to be a numpy.ndarray, a vector of shape n * 1.\n",
    "    \n",
    "    Returns:\n",
    "    The L2 regularization as a float.\n",
    "    None if theta in an empty numpy.ndarray.\n",
    "    \n",
    "    Raises:\n",
    "    This function should not raise any Exception.\n",
    "    \"\"\"\n",
    "\n",
    "    l2_reg = 0.0\n",
    "    for i in range(1, theta.shape[0]):\n",
    "        l2_reg += theta[i, 0] ** 2\n",
    "    return l2_reg\n",
    "\n",
    "def l2(theta):\n",
    "    \"\"\"\n",
    "    Computes the L2 regularization of a non-empty numpy.ndarray, without any for-loop.\n",
    "    \n",
    "    Args:\n",
    "    theta: has to be a numpy.ndarray, a vector of shape n * 1.\n",
    "    \n",
    "    Returns:\n",
    "    The L2 regularization as a float.\n",
    "    None if theta in an empty numpy.ndarray.\n",
    "    \n",
    "    Raises:\n",
    "    This function should not raise any Exception.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sum(theta[1:] * theta[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13505a5-1881-453d-852f-c3a6ef87dadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2, 14, -13, 5, 12, 4, -19]).reshape((-1, 1))\n",
    "\n",
    "iterative_l2(x) \n",
    "# Output: 911.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52050ec5-d549-47a8-86db-0772cc75b41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abd070-c63a-485d-b7f8-c702d3bd102c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.25"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([3,0.5,-6]).reshape((-1, 1))\n",
    "iterative_l2(y)\n",
    "# Output: 36.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216be64f-e0b3-4298-a433-22fe9ac9437a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.25"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2(y)\n",
    "# Output: 36.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d03ee5-8398-42da-af8c-342b7aed54d9",
   "metadata": {},
   "source": [
    "# Exercise 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab00b2-17c5-49a7-80f3-1d6a6b3ea672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_loss_(y, y_hat, theta, lambda_):\n",
    "    \"\"\"Computes the regularized loss of a linear regression model from two non-empty numpy.array, without any for loop.Args:\n",
    "    y: has to be an numpy.ndarray, a vector of shape m * 1.\n",
    "    y_hat: has to be an numpy.ndarray, a vector of shape m * 1.\n",
    "    theta: has to be a numpy.ndarray, a vector of shape n * 1.\n",
    "    lambda_: has to be a float.\n",
    "    Returns:\n",
    "    The regularized loss as a float.\n",
    "    None if y, y_hat, or theta are empty numpy.ndarray.\n",
    "    None if y and y_hat do not share the same shapes.\n",
    "    Raises:\n",
    "    This function should not raise any Exception.\n",
    "    \"\"\"\n",
    "    m = len(y_hat)\n",
    "    loss = 1/(2*m) * ( ((y_hat - y)**2).mean() + lambda_*l2(theta))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56d151-5785-4579-ac54-4010d5c6358a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40647959183673465"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([2, 14, -13, 5, 12, 4, -19]).reshape((-1, 1))\n",
    "y_hat = np.array([3, 13, -11.5, 5, 11, 5, -20]).reshape((-1, 1))\n",
    "theta = np.array([1, 2.5, 1.5, -0.9]).reshape((-1, 1))\n",
    "\n",
    "reg_loss_(y, y_hat, theta, 0.5)\n",
    "# Output: 0.8503571428571429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746ade2-2aaf-48d2-a739-dd8d706f31ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1072295918367347"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_loss_(y, y_hat, theta, 0.05)\n",
    "# Output: 0.5511071428571429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5bc01-d403-460d-8606-650e185672e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6724795918367348"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_loss_(y, y_hat, theta, 0.9)\n",
    "# Output: 1.116357142857143"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdcefb-0463-4c46-99be-96d5fe40ef35",
   "metadata": {},
   "source": [
    "# Exercise 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5812410-75c7-442c-9d5e-daf12a48fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_log_loss_(y, y_hat, theta, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the regularized loss of a logistic regression model from two non-empty numpy.ndarray, without any for loops\n",
    "    \n",
    "    Args:\n",
    "    y: has to be an numpy.ndarray, a vector of shape m * 1.\n",
    "    y_hat: has to be an numpy.ndarray, a vector of shape m * 1.\n",
    "    theta: has to be a numpy.ndarray, a vector of shape n * 1.\n",
    "    lambda_: has to be a float.\n",
    "    \n",
    "    Returns:\n",
    "    The regularized loss as a float.\n",
    "    None if y, y_hat, or theta is empty numpy.ndarray.\n",
    "    None if y and y_hat do not share the same shapes.\n",
    "    \n",
    "    Raises:\n",
    "    This function should not raise any Exception.\n",
    "    \"\"\"\n",
    "\n",
    "    m = y.shape[0]\n",
    "    loss =  -1/m * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) \n",
    "    l2_reg = 1/(2*m) * lambda_*l2(theta) \n",
    "    return  loss + l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c02cf-32ad-4294-a57b-fee6aed4296a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43377043716476066"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([1, 1, 0, 0, 1, 1, 0]).reshape((-1, 1))\n",
    "y_hat = np.array([.9, .79, .12, .04, .89, .93, .01]).reshape((-1, 1))\n",
    "theta = np.array([1, 2.5, 1.5, -0.9]).reshape((-1, 1))\n",
    "reg_log_loss_(y, y_hat, theta, .5)\n",
    "# Output: 0.43377043716475955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b995a5-ffaf-4730-962b-398325df7d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13452043716476064"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example :\n",
    "reg_log_loss_(y, y_hat, theta, .05)\n",
    "# Output: 0.13452043716475953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdb2a4-2d73-4d6d-84f4-bdcd33ebc35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6997704371647606"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example :\n",
    "reg_log_loss_(y, y_hat, theta, .9)\n",
    "# Output: 0.6997704371647596"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ada9e-8a38-408c-847f-149f52bf08b1",
   "metadata": {},
   "source": [
    "# Exercise 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a0412-3a62-4b5f-b312-e6368d4166d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_linear_grad(y, x, theta, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the regularized linear gradient of three non-empty numpy.ndarray,\n",
    "    with two for-loop. The three arrays must have compatible shapes.\n",
    "    \n",
    "    Args:\n",
    "    y: has to be a numpy.ndarray, a vector of shape m * 1.\n",
    "    x: has to be a numpy.ndarray, a matrix of dimesion m * n.\n",
    "    theta: has to be a numpy.ndarray, a vector of shape (n + 1) * 1.\n",
    "    lambda_: has to be a float.\n",
    "    \n",
    "    Return:\n",
    "    A numpy.ndarray, a vector of shape (n + 1) * 1, containing the results of the formula for all j.\n",
    "    None if y, x, or theta are empty numpy.ndarray.\n",
    "    None if y, x or theta does not share compatibles shapes.\n",
    "    None if y, x or theta or lambda_ is not of the expected type.\n",
    "    \n",
    "    Raises:\n",
    "    This function should not raise any Exception.\n",
    "    \"\"\"\n",
    "    \n",
    "    m, n = x.shape\n",
    "    X = np.c_[np.ones((m, 1)), x]\n",
    "    theta_t = np.copy(theta)\n",
    "    theta_t[0] = 0\n",
    "    grad = np.zeros((n + 1, 1))\n",
    "    \n",
    "    for j in range(n + 1):\n",
    "        for i in range(m):\n",
    "            grad[j] += (X[i, :].dot(theta) - y[i]) * X[i, j]\n",
    "        if j != 0:  # Skip regularization for j = 0\n",
    "            grad[j] += lambda_ * theta_t[j]\n",
    "        grad[j] /= m\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def vec_reg_linear_grad(y, x, theta, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the regularized linear gradient of three non-empty numpy.ndarray,\n",
    "    without any for-loop. The three arrays must have compatible shapes.\n",
    "    \n",
    "    Args:\n",
    "    y: has to be a numpy.ndarray, a vector of shape m * 1.\n",
    "    x: has to be a numpy.ndarray, a matrix of dimesion m * n.\n",
    "    theta: has to be a numpy.ndarray, a vector of shape (n + 1) * 1.\n",
    "    lambda_: has to be a float.\n",
    "    \n",
    "    Return:\n",
    "    A numpy.ndarray, a vector of shape (n + 1) * 1, containing the results of the formula for all j.\n",
    "    None if y, x, or theta are empty numpy.ndarray.\n",
    "    None if y, x or theta does not share compatibles shapes.\n",
    "    None if y, x or theta or lambda_ is not of the expected type.\n",
    "    \n",
    "    Raises:\n",
    "    This function should not raise any Exception.\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = x.shape\n",
    "    X = np.c_[np.ones((m, 1)), x]\n",
    "    theta_t = np.copy( theta)\n",
    "    theta_t[0] = 0\n",
    "    return 1/m * (X.T@(X@theta - y) + (lambda_ * theta_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b634f-cd1a-4a4a-bb37-518a693aaf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -60.99      ],\n",
       "       [-195.64714286],\n",
       "       [ 863.46571429],\n",
       "       [-644.52142857]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [ -6, -7, -9],\n",
    "    [ 13, -2, 14],\n",
    "    [ -7, 14, -1],\n",
    "    [ -8, -4, 6],\n",
    "    [ -5, -9, 6],\n",
    "    [ 1, -5, 11],\n",
    "    [ 9, -11, 8]])\n",
    "y = np.array([[2], [14], [-13], [5], [12], [4], [-19]])\n",
    "theta = np.array([[7.01], [3], [10.5], [-6]])\n",
    "reg_linear_grad(y, x, theta, 1)\n",
    "# Output: array([[ -60.99 ],[-195.64714286],[ 863.46571429], [-644.52142857]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd8d72-0e93-4eb3-8d25-513f3a5daa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -60.99      ],\n",
       "       [-195.64714286],\n",
       "       [ 863.46571429],\n",
       "       [-644.52142857]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_reg_linear_grad(y, x, theta, 1)\n",
    "# Output: array([[ -60.99 ],[-195.64714286],[ 863.46571429],[-644.52142857]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72ff0b-d301-44d0-ba62-ba1258c7e7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -60.99      ],\n",
       "       [-195.86142857],\n",
       "       [ 862.71571429],\n",
       "       [-644.09285714]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_linear_grad(y, x, theta, 0.5)\n",
    "# Output: array([[ -60.99 ], [-195.86142857], [ 862.71571429], [-644.09285714]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc9f022-50f7-4525-9987-34f5c007bebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -60.99      ],\n",
       "       [-195.86142857],\n",
       "       [ 862.71571429],\n",
       "       [-644.09285714]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_reg_linear_grad(y, x, theta, 0.5)\n",
    "# Output:array([[ -60.99 ],[-195.86142857],[ 862.71571429],[-644.09285714]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1ae30-f0b2-4480-816f-64a504087775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -60.99      ],\n",
       "       [-196.07571429],\n",
       "       [ 861.96571429],\n",
       "       [-643.66428571]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_linear_grad(y, x, theta, 0.0)\n",
    "# Output:array([[ -60.99 ],[-196.07571429],[ 861.96571429],[-643.66428571]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edddc0ac-b69a-42fd-affe-e481b0a5fb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -60.99      ],\n",
       "       [-196.07571429],\n",
       "       [ 861.96571429],\n",
       "       [-643.66428571]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_reg_linear_grad(y, x, theta, 0.0)\n",
    "# Output:array([[ -60.99 ],[-196.07571429],[ 861.96571429],[-643.66428571]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6e3e7-fdc6-4a75-b2de-3b1cd3d811e8",
   "metadata": {},
   "source": [
    "# Exercise 05 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c53da-fa86-4c5c-964e-53e032e926ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_(x): return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logistic_predict_(x, theta):\n",
    "    m,n = x.shape\n",
    "    x_bias = np.c_[np.ones((m, 1)), x]\n",
    "    return sigmoid_(np.dot(x_bias, theta))\n",
    "\n",
    "def reg_logistic_grad(y, x, theta, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the regularized logistic gradient of three non-empty numpy.ndarray, with two for-loops. The three arrayArgs:\n",
    "    y: has to be a numpy.ndarray, a vector of shape m * 1.\n",
    "    x: has to be a numpy.ndarray, a matrix of dimesion m * n.\n",
    "    theta: has to be a numpy.ndarray, a vector of shape n * 1.\n",
    "    lambda_: has to be a float.\n",
    "    \n",
    "    Returns:\n",
    "    A numpy.ndarray, a vector of shape n * 1, containing the results of the formula for all j.\n",
    "    None if y, x, or theta are empty numpy.ndarray.\n",
    "    None if y, x or theta does not share compatibles shapes.\n",
    "    \n",
    "    Raises:\n",
    "    This function should not raise any Exception.\n",
    "    \"\"\"\n",
    "    \n",
    "    m, n = x.shape\n",
    "    X = np.c_[np.ones((m, 1)), x]\n",
    "    theta_t = np.copy(theta)\n",
    "    theta_t[0] = 0\n",
    "    grad = np.zeros((n + 1, 1))\n",
    "    for j in range(n + 1):\n",
    "        for i in range(m):\n",
    "            grad[j] += (sigmoid_(X[i, :] @ theta) - y[i]) * X[i, j]\n",
    "        if j != 0: grad[j] += lambda_ * theta_t[j]\n",
    "    grad = grad / m\n",
    "    return grad\n",
    "        \n",
    "\n",
    "def vec_reg_logistic_grad(y, x, theta, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the regularized logistic gradient of three non-empty numpy.ndarray, without any for-loop. The three arrArgs:\n",
    "    y: has to be a numpy.ndarray, a vector of shape m * 1.\n",
    "    x: has to be a numpy.ndarray, a matrix of shape m * n.\n",
    "    theta: has to be a numpy.ndarray, a vector of shape n * 1.\n",
    "    lambda_: has to be a float.\n",
    "    \n",
    "    Returns:\n",
    "    A numpy.ndarray, a vector of shape n * 1, containing the results of the formula for all j.\n",
    "    None if y, x, or theta are empty numpy.ndarray.\n",
    "    None if y, x or theta does not share compatibles shapes.\n",
    "    \n",
    "    Raises:\n",
    "    This function should not raise any Exception.\n",
    "    \"\"\"\n",
    "    \n",
    "    m, n = x.shape\n",
    "    X = np.c_[np.ones((m, 1)), x]\n",
    "    theta_t = np.copy( theta)\n",
    "    theta_t[0] = 0\n",
    "    return 1/m * (X.T@(logistic_predict_(x, theta) - y) + (lambda_ * theta_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c442ee7-d20b-411f-baa2-a59410cb4d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.55711039],\n",
       "       [-1.40334809],\n",
       "       [-1.91756886],\n",
       "       [-2.56737958],\n",
       "       [-3.03924017]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0, 2, 3, 4],\n",
    "              [2, 4, 5, 5],\n",
    "              [1, 3, 2, 7]])\n",
    "y = np.array([[0], [1], [1]])\n",
    "theta = np.array([[-2.4], [-1.5], [0.3], [-1.4], [0.7]])\n",
    "\n",
    "reg_logistic_grad(y, x, theta, 1)\n",
    "# Output: array([[-0.55711039],[-1.40334809],[-1.91756886],[-2.56737958],[-3.03924017]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f7d8b-8e97-407f-b8e8-67c79032e8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.55711039],\n",
       "       [-1.40334809],\n",
       "       [-1.91756886],\n",
       "       [-2.56737958],\n",
       "       [-3.03924017]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1.2:\n",
    "vec_reg_logistic_grad(y, x, theta, 1)\n",
    "# Output:array([[-0.55711039],[-1.40334809],[-1.91756886],[-2.56737958],[-3.03924017]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60240330-d937-41cc-a24a-5d8a65b78441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.55711039],\n",
       "       [-1.15334809],\n",
       "       [-1.96756886],\n",
       "       [-2.33404624],\n",
       "       [-3.15590684]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_logistic_grad(y, x, theta, 0.5)\n",
    "# Output: array([[-0.55711039], [-1.15334809], [-1.96756886], [-2.33404624], [-3.15590684]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63c823-985e-4120-b8b6-8a5c700575be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.55711039],\n",
       "       [-1.15334809],\n",
       "       [-1.96756886],\n",
       "       [-2.33404624],\n",
       "       [-3.15590684]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_reg_logistic_grad(y, x, theta, 0.5)\n",
    "# Output: array([[-0.55711039], [-1.15334809],[-1.96756886], [-2.33404624], [-3.15590684]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c20093-6ef7-4570-8752-64482c317c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.55711039],\n",
       "       [-0.90334809],\n",
       "       [-2.01756886],\n",
       "       [-2.10071291],\n",
       "       [-3.27257351]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_logistic_grad(y, x, theta, 0.0)\n",
    "# Output: array([[-0.55711039], [-0.90334809],[-2.01756886], [-2.10071291], [-3.27257351]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca47d3-5bc6-4d42-b244-0bdb2dc26611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.55711039],\n",
       "       [-0.90334809],\n",
       "       [-2.01756886],\n",
       "       [-2.10071291],\n",
       "       [-3.27257351]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_reg_logistic_grad(y, x, theta, 0.0)\n",
    "# Output: array([[-0.55711039], [-0.90334809], [-2.01756886], [-2.10071291], [-3.27257351]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d9c88-93c5-4cf2-9e7d-97823c6313d5",
   "metadata": {},
   "source": [
    "# Exercise 06 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca70e1b-6664-42fa-9a86-cb786284642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    My personnal linear regression class to fit like a boss.\n",
    "    \"\"\"\n",
    "    def __init__(self, thetas, alpha=0.001, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.thetas = thetas\n",
    "\n",
    "    def fit_(self, x, y):\n",
    "        self.thetas = theta.astype(np.float64)\n",
    "        for i in range(self.max_iter):\n",
    "            grad = simple_gradient(x, y, self.thetas)\n",
    "            self.thetas-= grad.astype(np.float64) * self.alpha\n",
    "        return self.thetas\n",
    "\n",
    "    def add_intercept(self, x):\n",
    "        \"\"\"\n",
    "        Adds a column of 1’s to the non-empty numpy.array x.\n",
    "        \n",
    "        Args:\n",
    "        x: has to be a numpy.array of dimension m * n.\n",
    "        \n",
    "        Returns:\n",
    "        X, a numpy.array of dimension m * (n + 1).\n",
    "        None if x is not a numpy.array.\n",
    "        None if x is an empty numpy.array.\n",
    "        \n",
    "        Raises:\n",
    "        This function should not raise any Exception.\n",
    "        \"\"\"\n",
    "    \n",
    "        if not isinstance(x, np.ndarray) or x.size == 0:\n",
    "            return None\n",
    "    \n",
    "        if len(x.shape) == 1: x = x.reshape(x.shape[0], 1)\n",
    "        \n",
    "        ones = np.ones((x.shape[0], 1))\n",
    "        X = np.hstack((ones, x))\n",
    "        return X\n",
    "    \n",
    "    def predict_(self, x):\n",
    "        \"\"\"\n",
    "        Computes the vector of prediction y_hat from two non-empty numpy.array.\n",
    "        \n",
    "        Args:\n",
    "        x: has to be an numpy.array, a vector of dimension m * 1.\n",
    "        theta: has to be an numpy.array, a vector of dimension 2 * 1.\n",
    "        \n",
    "        Returns:\n",
    "        y_hat as a numpy.array, a vector of dimension m * 1.\n",
    "        None if x and/or theta are not numpy.array.\n",
    "        None if x or theta are empty numpy.array.\n",
    "        None if x or theta dimensions are not appropriate.\n",
    "        \n",
    "        Raises:\n",
    "        This function should not raise any Exceptions.\n",
    "        \"\"\"\n",
    "    \n",
    "        if not (isinstance(x, np.ndarray) and isinstance(self.thetas, np.ndarray)):\n",
    "            return None\n",
    "    \n",
    "        if x.size == 0 or self.thetas.size == 0:\n",
    "            return None\n",
    "    \n",
    "        if self.thetas.shape != (2,1):\n",
    "             return None\n",
    "\n",
    "        return self.add_intercept(x) @ self.thetas\n",
    "\n",
    "    \n",
    "    def loss_elem_(self, y, y_hat):\n",
    "        if not (isinstance(y, np.ndarray) and isinstance(y_hat, np.ndarray)):\n",
    "            return None\n",
    "        \n",
    "        if y.shape != y_hat.shape:\n",
    "            return None\n",
    "    \n",
    "        J_elem = np.zeros(y.shape)\n",
    "        for i in range(len(y)):\n",
    "            J_elem[i] = (y[i] - y_hat[i]) ** 2\n",
    "\n",
    "        return J_elem\n",
    "        \n",
    "    def loss_(self, y, y_hat):\n",
    "        \n",
    "        J_elem = self.loss_elem_(y, y_hat)\n",
    "        if J_elem is None:\n",
    "            return None\n",
    "    \n",
    "        J_value = 0.0\n",
    "        for elem in J_elem:\n",
    "            J_value += elem\n",
    "        J_value /= (2 * len(J_elem))\n",
    "    \n",
    "        return float(J_value)\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_(y, y_hat):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Calculate the MSE between the predicted output and the real output.\n",
    "        \n",
    "        Args:\n",
    "        y: has to be a numpy.array, a vector of dimension m * 1.\n",
    "        y_hat: has to be a numpy.array, a vector of dimension m * 1.\n",
    "        \n",
    "        Returns:\n",
    "        mse: has to be a float.\n",
    "        None if there is a matching dimension problem.\n",
    "        \n",
    "        Raises:\n",
    "        This function should not raise any Exceptions.\n",
    "        \"\"\"\n",
    "        return ((y - y_hat) ** 2).mean()\n",
    "\n",
    "    @classmethod\n",
    "    def rmse_(y, y_hat):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Calculate the RMSE between the predicted output and the real output.\n",
    "        Args:\n",
    "        y: has to be a numpy.array, a vector of dimension m * 1.\n",
    "        y_hat: has to be a numpy.array, a vector of dimension m * 1.\n",
    "        Returns:\n",
    "        rmse: has to be a float.\n",
    "        None if there is a matching dimension problem.\n",
    "        Raises:\n",
    "        This function should not raise any Exceptions.\n",
    "        \"\"\"\n",
    "        return sqrt(mse_(y, y_hat))\n",
    "    \n",
    "    @classmethod\n",
    "    def mae_(y, y_hat):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Calculate the MAE between the predicted output and the real output.\n",
    "        Args:\n",
    "        y: has to be a numpy.array, a vector of dimension m * 1.\n",
    "        y_hat: has to be a numpy.array, a vector of dimension m * 1.\n",
    "        Returns:\n",
    "        mae: has to be a float.\n",
    "        None if there is a matching dimension problem.\n",
    "        Raises:\n",
    "        This function should not raise any Exceptions.\n",
    "        \"\"\"\n",
    "        return np.abs(y - y_hat).mean()\n",
    "\n",
    "    @classmethod\n",
    "    def r2score_(y, y_hat):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Calculate the R2score between the predicted output and the output.\n",
    "        Args:\n",
    "        y: has to be a numpy.array, a vector of dimension m * 1.\n",
    "        y_hat: has to be a numpy.array, a vector of dimension m * 1.\n",
    "        Returns:\n",
    "        r2score: has to be a float.\n",
    "        None if there is a matching dimension problem.\n",
    "        Raises:\n",
    "        This function should not raise any Exceptions.\n",
    "        \"\"\"\n",
    "        return 1 - (((y - y_hat) ** 2).sum() / ((y - y.mean()) ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce03be-91ae-424b-b196-7ff614515f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRidge(MyLinearRegression):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    My personal ridge regression class to fit like a boss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, thetas, alpha=0.001, max_iter=1000, lambda_=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the MyRidge object.\n",
    "\n",
    "        Parameters:\n",
    "        - thetas: numpy array of shape (n_features,) representing the initial model coefficients\n",
    "        - alpha: learning rate for gradient descent (default: 0.001)\n",
    "        - max_iter: maximum number of iterations for gradient descent (default: 1000)\n",
    "        - lambda_: regularization parameter (default: 0.5)\n",
    "        \"\"\"\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.thetas = thetas\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def get_params_(self):\n",
    "        \"\"\"\n",
    "        Get the parameters of the estimator.\n",
    "\n",
    "        Returns:\n",
    "        - params: dictionary containing the parameters of the estimator\n",
    "        \"\"\"\n",
    "\n",
    "        return self.thetas\n",
    "\n",
    "    def set_params_(self, **params):\n",
    "        \"\"\"\n",
    "        Set the parameters of the estimator.\n",
    "\n",
    "        Parameters:\n",
    "        - params: dictionary containing the new parameter values\n",
    "        \"\"\"\n",
    "        # self.thetas = \n",
    "\n",
    "    def loss_(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the loss between two vectors.\n",
    "\n",
    "        Parameters:\n",
    "        - y_true: numpy array of shape (n_samples,) representing the true values\n",
    "        - y_pred: numpy array of shape (n_samples,) representing the predicted values\n",
    "\n",
    "        Returns:\n",
    "        - loss: float value representing the loss\n",
    "        \"\"\"\n",
    "\n",
    "        return reg_loss_(y_true, y_pred , self.thetas, self.lambda_)\n",
    "\n",
    "    def loss_elem_(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate a vector corresponding to the squared difference between two vectors.\n",
    "\n",
    "        Parameters:\n",
    "        - y_true: numpy array of shape (n_samples,) representing the true values\n",
    "        - y_pred: numpy array of shape (n_samples,) representing the predicted values\n",
    "\n",
    "        Returns:\n",
    "        - loss_elem: numpy array of shape (n_samples,) representing the element-wise squared difference\n",
    "        \"\"\"\n",
    "\n",
    "        return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "    def gradient_(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the vectorized regularized gradient.\n",
    "\n",
    "        Parameters:\n",
    "        - X: numpy array of shape (n_samples, n_features) representing the input features\n",
    "        - y: numpy array of shape (n_samples,) representing the true values\n",
    "\n",
    "        Returns:\n",
    "        - gradient: numpy array of shape (n_features,) representing the gradient\n",
    "        \"\"\"\n",
    "\n",
    "        return vec_reg_linear_grad(y, X, self.thetas, self.lambda_)\n",
    "\n",
    "    def fit_(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Ridge regression model to a training dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - X: numpy array of shape (n_samples, n_features) representing the input features\n",
    "        - y: numpy array of shape (n_samples,) representing the true values\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            grad = self.gradient_(X, y)\n",
    "            self.thetas-= grad * self.alpha\n",
    "        return self.thetas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363d91e5-878b-40e0-861d-2e7c41f554e8",
   "metadata": {},
   "source": [
    "# Exercise 07 : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855fbf7-9198-4180-869d-8414275c4e5e",
   "metadata": {},
   "source": [
    "# Exercise 08 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8d7d9-d304-494d-9281-526762aca9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9179da8-4ac4-4076-8d06-7aab3d4eb726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    My personnal logistic regression to classify things.\n",
    "    \"\"\"\n",
    "    supported_penalities = ['l2'] # We consider l2 penality only. One may wants to implement other penalities\n",
    "    def __init__(self, theta, alpha=0.001, max_iter=1000, penality='l2', lambda_=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.theta = theta\n",
    "        self.penality = penality\n",
    "        self.lambda_ = lambda_ if penality in self.supported_penalities else 0\n",
    "\n",
    "    def predict_(self, x):\n",
    "        \n",
    "        def sigmoid_(x): return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        m,n = x.shape\n",
    "        x_bias = np.c_[np.ones((m, 1)), x]\n",
    "        return sigmoid_(np.dot(x_bias, self.theta))\n",
    "        \n",
    "    def loss_elem_(self, y, yhat):\n",
    "        return reg_log_loss_(y, y_hat, self.thetas, self.lambda_)\n",
    "\n",
    "\n",
    "    def loss_(self, X, y):\n",
    "        y_hat = self.predict_(X)\n",
    "        eps = 1e-15\n",
    "        m = y.shape[0]\n",
    "        y_hat = np.clip(y_hat, eps, 1 - eps)\n",
    "\n",
    "        return reg_log_loss_(y, y_hat, self.thetas, self.lambda_)\n",
    "    \n",
    "    \n",
    "    def fit_(self, x, y):\n",
    "        \n",
    "        def vec_log_gradient(x, y, theta):\n",
    "            m, n = x.shape\n",
    "            X = np.c_[np.ones((m, 1)), x]\n",
    "            return 1/m * X.T@(self.predict_(x) - y)\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            grad = vec_reg_linear_grad(y, x, self.thetas, self.lambda_)\n",
    "            self.theta -= grad * self.alpha\n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11aeba7-b6fd-4b8c-87d7-cc188ae488e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "theta = np.array([[-2.4], [-1.5], [0.3], [-1.4], [0.7]])\n",
    "model1 = MyLogisticRegression(theta, lambda_=5.0)\n",
    "print(model1.penality) # Output ’l2’\n",
    "print(model1.lambda_) # Output 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92193cc-c434-4e40-8f29-43a5c7e82a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "model2 = MyLogisticRegression(theta, penality=None)\n",
    "print(model2.penality) # Output None\n",
    "print(model2.lambda_) # Output 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821043f-772a-4d7d-803b-8e132ca8212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = mylogr(theta, penality=None, lambda_=2.0)\n",
    "model3.penality # Output None\n",
    "model3.lambda_ # Output 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
